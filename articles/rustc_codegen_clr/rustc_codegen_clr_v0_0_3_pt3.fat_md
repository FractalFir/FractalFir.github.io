<metadata>
title = "Mischievous miscompilations"
id = "rustc_codegen_clr_v0_0_3_3"
category = "Rustc Codegen CLR"
date = "30 Oct 2023"
</metadata>
<markdown>
*This is the 4th part of my series about writing a [Rust compiler backend targeting .NET](https://github.com/FractalFir/rustc_codegen_clr)*

Writing even a part of a compiler is not a trivial task, and it is quite easy to mess things up. In this article, I will explain some of the challenges arising from the quirks of both CIL(.NET's assembly format) and MIR (Rust intermediate 
representation). I will also talk a little bit about leveraging some more interesting properties of CIL to catch errors automatically, and about how I doubled the current test coverage. 

# How does CIL work?

Before going more in depth into the exact causes of errors and some of the issues I commonly encountered while translating Rust into .NET assemblies, I should probably explain some basics of CIL, in order to make more complicated examples easier to 
digest.

## Stack-based

The first important property of CIL is that it is stack-based. It describes a sequence of operations on a stack of an imaginary CPU, and the JIT then turns instructions for this made-up processor into heavily optimized native code. This may seem
quite weird at first, since almost all CPUs tend to operate on registers, with the [few exceptions](https://en.wikipedia.org/wiki/Stack_machine) to this rule being little more than oddities in some computer history museums. There are a few 
advantages to this form of intermediate representation. For one, it assumes nothing about how many registers the target CPU has, making it quite portable. By not specifying anything about registers and relying solely on the JIT to do the hard
job of selecting the right ones, it also avoids having to deal with all the architecture-specific weirdness, such as floating point registers being sometimes different from general purpose ones. 

Another huge advantage of stack-based IL is the reduction in assembly size. This is not a concern for languages which are AOT compiled to native code, since the IL is present only during compilation, but it is crucial for .NET, where all assemblies
contain CIL.

In order to showcase both the assembly size savings and to further explain CIL, let us consider a simple function: `add`. It takes two numbers, `a` and `b`, and returns their sum. This is how it looks like in CIL:
```cil
.method public static int32 sum(int32,int32){
	// Load 0th argument on the stack 
	ldarg.0
	// Load 1st argument on the stack 
	ldarg.1
	// Add the top 2 numbers on the stack, push sum on the stack
	add
	// Return the top value on the stack
	ret
}
```
How many bytes do you think it takes to encode the CIL inside this method?

Exactly 4 bytes. The word `byte` takes the same amount of space as the code inside this function. Rust MIR or LLVM IR - compile-time representations which preform arithmetic's directly on variables, would take this much space just to encode 
**one of the variables the operation is performed on**. They can afford to waste space - they are temporary, after all. But .NET's IL must be much more resource conservative, since it is stored permanently. 

## The behavior depends on context.

This is another of CIL's space-saving schemes. One I am much less fond of, since it caused me quite a bit of headaches. What is it exactly? 

Well, some instructions do slightly different things depending on other instructions around them. Where in Java bytecode (which is quite similar to .NET IL) there are `dadd`, `fadd`, `iadd`, `ladd` instructions to add doubles, floats, integers and 
longs respectively, CIL has just one `add` instruction. The same can be said for the `ldarg` and `ldloc` instructions. While JVM treats arguments as local variables, using the same instructions to access them, it still has way more variants of 
its `local` instruction. To showcase the huge advantages of this type deduction, I will now show the variant of the previously shown `sum` CIL function. This version adds doubles instead of 32-bit integers:
```cil
 .method public static float64 sum(float64,float64){
	// Load 0th argument on the stack 
	ldarg.0
	// Load 1st argument on the stack 
	ldarg.1
	// Add the top 2 numbers on the stack, push sum on the stack
	add
	// Return the top value on the stack
	ret
}
```
The body of the function did not have to change at all - changing the function signature was more than enough. By reusing the same `add` instruction for adding multiple different types, CIL is able to assign more variants to instructions such as
`ldloc`. The `ldloc.0` and `ldloc.1` variants I use are actually short forms of the ldloc instruction. Instead of using an op followed by 16 bit ushort(u16) to encode loading of the locals 0 to 4, dedicated, 1-byte wide variants allow us to save
over 60% of space! There is also a variant which consists of an op followed by a single byte(u8), which is used to encode accessing locals in range 5-255. This too saves quite a bit of space. As you can see, by not wasting space for dedicated, type
specific variants of arithmetic's, we can reduce the size of all assemblies by a lot. 

So, what is the problem?

## The behavior depends on context...

There are cases where the runtime deduces types in a way that is a bit... unusual. It is still fully correct and specified. The issue is not that it is wrong or dumb - it is too smart. There are some cases where I would expect something to be invalid
and cause the JIT to throw an exception - but the runtime just happily chugged along. I want to reiterate - this is **not an issue with the runtime**. It is just me being stupid. 

So, what was the problem?

After compiling one of my tests in debug mode(I added partial support for Rust debug compilation - more on this later), it began to fail with a null deference exception. This was a miss-compilation, which was quite complex and spanned multiple 
functions. Adding the details on where exactly what was called would only make this hard to follow - so let me save you the headache and simplify the issue.

The root cause was part of field access code using `ldfld` instruction instead of `ldflda` when attempting to get the address of a primitive field. This in turn caused the compiled code to incorrectly load the *value* of an `int32` field instead of 
a pointer to it. The problematic CIL looked something like this:
```cil
// load the value of field `int Somefield` on the stack
ldfld int32 SomeType::Somefield
// load the integer behind the pointer on top of the stack
ldind.i4 
```
when it should have looked like this:
```cil
// load the address of field `int Somefield` on the stack
ldflda int32 SomeType::Somefield
// load the integer behind the pointer on top of the stack
ldind.i4 
```

One letter difference - hard to spot even when all else is removed. I would have expected this to fail - after all, 32bit integers and 64 bit pointers are completely different things. The runtime promoted the `int` to a `int*`, turning a value of 
`0` into a null pointer. I would expect it to throw an exception and refuse to compile such a thing - requiring an explicit `conv.i` instruction to change an int to a pointer. 

Looking back at this, the behavior makes sense - in a weird, contrived  way. Why use an additional instruction to do such a thing, where the behavior can be so easily deduced? This is an example where runtime being a bit too smart results in a lot of 
pain for me. Normal users won't ever encounter this - the C# compiler will easily catch such things - but I am *writing* a compiler, so there is none to save me from my own stupidity. 

What is even worse, is that this is fully valid CIL. I can't catch such kind of error inside the codegen, since such a combination of ops *could* be a result of compiling a valid Rust program. I would have to reject valid CIL, which is definitely 
not what I want to do.

There are, however, ways to catch some issues.

# Simulating the stack.

Due to the complexity of some parts of the codegen, it is quite easy to get lost in a sea of ops. Keeping track of what is on the stack is a nightmare once you go beyond trivial programs. It is not an uncommon occurrence for me to forget an op, and
mess the stack up completely. Accidentally load something twice, or not load it at all. How would one go about catching such issues? This is where one of the design decisions I made early on comes to my rescue.  

## MIR statements

Before going into how I catch such bugs, I must explain a little bit about Rust MIR. It is quite complicated, so I will focus only on what is strictly necessary to get the idea across. In MIR, functions consist of a series of statements. Statement is
the simplest, independent, meaningful piece of code. For example, this statement adds 2 numbers together: 
```
_1 = Add(move _2, move _3);
```
It assigns the local variable `_1` the value of `_2` + `_3`. What is important to notice is that every statement is such an assignment. The codegen "sees" only one statement at a time - this is done to prevent issues with compiling one of them from
affecting any other. This has its own cost - the codegen must spend more time optimizing the final CIL, and it requires a bit more work. It has the unique advantage of being really easy to debug and pinpoint any issues.

What is particularly interesting is that before the optimization step occurs, the stack is **always** empty before and after each statement. Since this will always hold, unless and issue pops up, we can simulate how each op in a statement affects
the stack. If during this simulation, the amount of things on the stack is ever below 0 or is not equal to 0 at the end, a miscompilation must have occurred. The codegen will then throw an error, with a message including the statement, what ops did it get
translated into, and step-by step breakdown on how each op affected the stack. This significantly simplifies the debugging process for this particular kind of miscompilation. Others still are not very pleasant to debug, but I managed to squash quite 
a bit of them.

# Building tests in debug mode

Another big change I worked on this week is the addition of support for building Rust programs in debug mode. One of the reasons I started with compiling only optimized Rust code was the added complexity of debug Rust features. Optimized Rust code 
does introduce the need to support certain features not present in debug mode, but it is far easier to disable optimizations than to implement things like bounds checking. You can just use `std::hint::black_box` to tell the compiler that it can't
optimize a particular operation. 

Making the test harness also run tests in debug mode was relatively straightforward, since it just required copying the existing test runners, and removing the "--release" option from the tester meant for debug code.

Getting the codegen to build debug-mode code is a different story entirely.

## Checked ops

Rust MIR has a special kind of arithmetic operation meant for debugging and certain safety-oriented applications: a checked operation, such as `CheckedAdd`.

.NET also has a similar feature, with opcodes such as `add.ovf` serving an almost identical function.

### Why I can't use built-in checked ops.

You would think that I could just use `add.ovf` and its friends to implement Rust's checked ops, right?  

Nope. 

The difference is pretty fundamental and comes from the different philosophy of error handling in .NET and Rust. 

#### Error handling - .NET vs Rust

One of the main differences between Rust and .NET is the error handling. .NET uses exceptions, which go up through the call stack until encountering a suitable handler. Rust on the other hand mainly uses `Result` types - more explicit mechanism which 
requires the one writing the code to exactly specify what happens to errors on each function call. Is the error handled in some way? Should the program crash? Or should the error be reported up to the caller? Exceptions vs Results is a complicated
topic in which the "best" option depends entirely on use case, and personal preferences. I tend to gravitate more towards `Result` - since it is easy to see which functions can fail (by just looking at their return type), and quite hard to forget to
handle it.

What is important for us is the performance impacts of this approach. .NET is an "optimist" - it assumes errors happen rarely. The cost of raising an exception is relatively high (thousand of ns), but the exception handling is "free" when errors don't
occur. Rust is a pessimist - the world is cruel and errors could happen anytime, so it is important to be just as fast in the case of an error as in the case of success. This can (due to compiler does not have too) have a tiny performance impact in 
each call. Comparing them directly is very tricky, since the Rust compiler sacrifices build speeds for a very precise analysis of the program, very often optimizing the costs of `Result` or `Option` away completely. An example of such a clever 
optimization is that `Option<&T>` fits in the same space as `&T` - since the null pointer value can be used to represent `None`. There are many other things which make all of this messy and countless articles arguing one or another way.

The point I am trying to make - this is not an apples to apples comparison, and it is almost never relevant. 

In our case, the issue is that Rust promises checked arithmetics to succeed and fail in roughly the same time. The error case is not special - it is just a return value, like `7` or `42`. The cost of raising an exception is significant when compared
to just adding 2 numbers - so this normally useful feature of .NET is useless for our case. The "unhappy" case overhead could break some Rust programs - precisely because they may rely on this errors-are-cheap behavior. 

#### Differences in checked arithmetics

While .NET's checked ops throw an exception on overflow, Rust's checked ops return a tuple `(bool,number_type)`, containing a boolean signaling that under/overflow occurred, and the result of the arithmetic operation. It may seem like I could just
use the .NET checked op and set the tuple to `(false,result)` after the operation, since in most cases a failed checked op will be followed by a panic anyway. The issue is that such an operation is not *always* followed by a panic, and is sometimes
used as part of an algorithm's logic. Doing some weirdness with `try`/`catch` would introduce quite a bit of unnecessary overhead, which would not be expected by the people writing the compiled Rust code. One of the more ambitious goals of the project
is to compile **any** Rust code to .NET. This could bring countless Rust crates (libraries) to .NET, allowing for easy use of code which preforms no managed allocations, freeing precious GC time. Because most authors of those crates did not write them
with this project and .NET in mind, the code compiled by the codegen should behave as closely to native Rust as possible. 

## Implementation of CheckedAdd

One of the big challenges is that I can't access things like the carry flag, which would make detecting overflows trivial, from CIL, since those sorts of things are architecture-specific. My algorithms have to be architecture-agnostic, and implemented
in pure CIL.

I currently have finished implementing checked addition for all unsigned integers and have a WIP version of checked addition of signed ints. 

Since this article is getting a bit long, and checked ops will be a major part of the next one, I won't get into the details of how things work under the hood (on CIL level). 

I will just show how the algorithm works, and explain the principles behind it.

### Unsigned checked add

```csharp
   var sum = a + b;
   bool overflown = sum < a | b;
```
For unsigned integers, a sum should be bigger than both `a` and `b`, unless we overflow. If the sum is smaller than the bitwise `or` of the values (which it never should be, due to how addition works on the binary level), this means we have overflown.
The exact implementation on CIL level is something I will leave for later. 
### Signed checked add
*This example uses 8-bit integers, but any other size will work too.*
```csharp
   sbyte sum = (sbyte)(a + b);
   sbyte sign_a = (sbyte)(a & 0b1000_0000);
   sbyte sign_b =(sbyte)(b & 0b1000_0000);
   sbyte sum_sign = (sbyte)(sum & 0b1000_0000);
   bool signs_equal = sign_a == sign_b;
   bool overflown = signs_equal && sum_sign != sign_a;
```
For detecting signed overflow, we can leverage 2 facts: 
First, only adding 2 variables with the same sign may overflow/underflow  - since if the signs are different, the absolute value of the result will be smaller than the bigger operand, so it can't overflow/underflow.

Secondly, if an overflow/underflow occurs, the sign of the result will be different from the sign of the operands.

Using those facts, we can detect all overflows in a decently performant, platform-agnostic way.

# Conclusion

I hope you enjoyed this article. It is a bit less focused on new features, but I hope it still was an enjoyable read.  

If you want, you can [check out the project](https://github.com/FractalFir/rustc_codegen_clr), take a look and maybe even contribute. 

Have a good day, and I hope I will see you again soon!
</markdown>
